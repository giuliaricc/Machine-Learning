import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import f_classif
from sklearn.linear_model import LinearRegression
from statsmodels.stats.outliers_influence import variance_inflation_factor
import os
import matplotlib.pyplot as plt
import seaborn as sns

# ---------------------
# STEP 1: Cleaning del train
# ---------------------

# Carica il dataset
train_data = pd.read_csv("/Users/davidecanfora/Desktop/Apprendimento automatico/Progetto/home-data-for-ml-course/train.csv")

# Rimuovo la colonna ID
train_data.drop(columns=['Id'], inplace=True)

# Aggiungo una colonna che conta i NaN per riga
train_data['na_count'] = train_data.isna().sum(axis=1)
print("Max NaN per riga:", train_data['na_count'].max())

# Rimuovo la colonna di conteggio dei NaN
train_data.drop(columns=['na_count'], inplace=True)

# Individuo le colonne con troppi NaN (> 40%)
threshold = 0.40 * train_data.shape[0]
colonne_con_troppi_NaN = train_data.columns[train_data.isna().sum() > threshold]
print("Colonne con troppi NaN (>40%):", colonne_con_troppi_NaN)

# Sostituzione dei NaN con valori specifici
train_data['Alley'].fillna('AlleyUnwanted', inplace=True)
train_data['FireplaceQu'].fillna('FireplaceUnwanted', inplace=True)
train_data['PoolQC'].fillna('PoolUnwanted', inplace=True)
train_data['Fence'].fillna('FenceUnwanted', inplace=True)
train_data['MiscFeature'].fillna('FeaturesUnwanted', inplace=True)
train_data['BsmtQual'].fillna('BasementUnwanted', inplace=True)
train_data['BsmtCond'].fillna('BasementUnwanted', inplace=True)
train_data['BsmtExposure'].fillna('BasementUnwanted', inplace=True)
train_data['BsmtFinType1'].fillna('BasementUnwanted', inplace=True)
train_data['BsmtFinType2'].fillna('BasementUnwanted', inplace=True)
train_data['GarageType'].fillna('GarageUnwanted', inplace=True)
train_data['GarageFinish'].fillna('GarageUnwanted', inplace=True)
train_data['GarageQual'].fillna('GarageUnwanted', inplace=True)
train_data['GarageCond'].fillna('GarageUnwanted', inplace=True)

# Converte le colonne object in categorie
train_data = train_data.astype({col: 'category' for col in train_data.select_dtypes(include='object').columns})

# Trova le variabili categoriche
categorical_vars = train_data.select_dtypes(include='category').columns
print(categorical_vars)

# ANOVA per individuare variabili categoriche significative
significant_categoricals = []
for var in categorical_vars:
    anova_result = f_classif(train_data[[var]].fillna('missing'), train_data['SalePrice'])
    p_value = anova_result[1][0]
    if p_value < 0.05:
        significant_categoricals.append(var)
    else:
        print(f"Non significativa: {var} con p-value: {p_value}")

# Encoding manuale delle variabili ordinali
encode_map = {
    "Po": 1, "Fa": 2, "TA": 3, "Gd": 4, "Ex": 5,
    "BasementUnwanted": 0, "FireplaceUnwanted": 0, "GarageUnwanted": 0, "PoolUnwanted": 0
}

ordinal_vars = [
    'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC',
    'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'
]

for var in ordinal_vars:
    train_data[var] = train_data[var].map(encode_map)

# One-Hot Encoding
encoder = OneHotEncoder(drop='first', sparse=False)
encoded_categorical = encoder.fit_transform(train_data[significant_categoricals])
encoded_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(significant_categoricals))

# Unisco le variabili trasformate al dataset
train_data = pd.concat([train_data, encoded_df], axis=1)
train_data.drop(columns=significant_categoricals, inplace=True)

# Rimuovo variabili con bassa correlazione (< 0.2)
cor_target = train_data.corr()['SalePrice'].abs().sort_values(ascending=False)
selected_vars = cor_target[cor_target > 0.2].index.tolist()

# Variance Inflation Factor (VIF) per multicollinearità
X = train_data[selected_vars].drop(columns=['SalePrice'])
vif = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
high_vif_vars = vif[vif > 10].index
train_data.drop(columns=high_vif_vars, inplace=True)

# Sostituisco i NaN con la media
train_data.fillna(train_data.mean(), inplace=True)

# ---------------------
# STEP 2: Cleaning del test
# ---------------------

test_data = pd.read_csv("/Users/davidecanfora/Desktop/Apprendimento automatico/Progetto/home-data-for-ml-course/test.csv")
test_data.drop(columns=['Id'], inplace=True)

# Stesse trasformazioni del training set
for var in ordinal_vars:
    test_data[var] = test_data[var].map(encode_map)

# One-Hot Encoding test
encoded_test = encoder.transform(test_data[significant_categoricals])
encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(significant_categoricals))

# Aggiungi le colonne mancanti
missing_cols = set(encoded_df.columns) - set(encoded_test_df.columns)
for col in missing_cols:
    encoded_test_df[col] = 0

# Ordino le colonne per essere coerente con il training set
encoded_test_df = encoded_test_df[encoded_df.columns]

# Combino il dataset test
test_data = pd.concat([test_data, encoded_test_df], axis=1)
test_data.drop(columns=significant_categoricals, inplace=True)

# Sostituisco i NaN con la media
test_data.fillna(test_data.mean(), inplace=True)

# ---------------------
# STEP 3: Check stesse colonne
# ---------------------
if set(test_data.columns) == set(train_data.columns):
    print("✅ I dataset hanno le stesse colonne.")
else:
    print("⚠️ I dataset NON hanno le stesse colonne.")

# ---------------------
# STEP 4: Salvo i dataset
# ---------------------
if not os.path.exists("dataset_progetto"):
    os.mkdir("dataset_progetto")

train_data.to_csv("dataset_progetto/save_train.csv", index=False)
test_data.to_csv("dataset_progetto/save_test.csv", index=False)

print("✅ I dataset sono stati salvati nella cartella 'dataset_progetto'.")
