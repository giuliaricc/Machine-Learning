import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.feature_selection import f_classif
from statsmodels.stats.outliers_influence import variance_inflation_factor
import os

# =============== Importazione dati ===============
# Importazione dati
train_file_path = 'C:\\Users\\giuli\\OneDrive\\Desktop\\ERASMUS\\Esami ENSIIE\\Modelisation Statistique\\TP2\\home-data-for-ml-course\\train.csv'
test_file_path = 'C:\\Users\\giuli\\OneDrive\\Desktop\\ERASMUS\\Esami ENSIIE\\Modelisation Statistique\\TP2\\home-data-for-ml-course\\test.csv'

train_data = pd.read_csv(train_file_path)
test_data = pd.read_csv(test_file_path)

train = train_data.copy()

# Rimuove la colonna ID
train_data = train_data.iloc[:, 1:]

# =============== Gestione NaN ===============
# Aggiunge una colonna con il numero di NaN per riga
train_data['na_count'] = train_data.isna().sum(axis=1)
print(f"Numero massimo di NaN per riga: {train_data['na_count'].max()}")
train_data.drop('na_count', axis=1, inplace=True)

# Rimuove colonne con più del 40% di valori NaN
threshold = 0.40 * len(train_data)
cols_to_drop = train_data.columns[train_data.isna().sum() > threshold]
train_data.drop(columns=cols_to_drop, inplace=True)

# Sostituzione di NaN con valori "unwanted"
columns_to_replace = {
    'Alley': "UNWANTED",
    'FireplaceQu': "UNWANTED",
    'PoolQC': "PoolUnwanted",
    'Fence': "FenceUnwanted",
    'MiscFeature': "FeaturesUnwanted",
    'BsmtQual': "BasementUnwanted",
    'BsmtCond': "BasementUnwanted",
    'BsmtExposure': "BasementUnwanted",
    'BsmtFinType1': "BasementUnwanted",
    'BsmtFinType2': "BasementUnwanted",
    'GarageType': "GarageUnwanted",
    'GarageFinish': "GarageUnwanted",
    'GarageQual': "GarageUnwanted",
    'GarageCond': "GarageUnwanted"
}

for col, value in columns_to_replace.items():
    if col in train_data.columns:
        train_data[col].fillna(value, inplace=True)

# =============== Conversione categoriche ===============
# Converte variabili categoriche in `category`
train_data = train_data.astype({col: 'category' for col in train_data.select_dtypes('object').columns})

# Variabili categoriche
categorical_vars = train_data.select_dtypes('category').columns

# =============== ANOVA per selezione variabili categoriche ===============
significant_categoricals = []
non_significant = []

for var in categorical_vars:
    if var in train_data.columns:
        le = LabelEncoder()
        encoded_var = le.fit_transform(train_data[var].astype(str))
        model = f_classif(encoded_var.reshape(-1, 1), train_data['SalePrice'])[1][0]
        if model < 0.05:
            significant_categoricals.append(var)
        else:
            non_significant.append((var, model))

print("Variabili categoriche significative:", significant_categoricals)

# =============== One-Hot Encoding ===============
encoder = OneHotEncoder(drop='first', sparse_output=False)
cat_one_hot = pd.DataFrame(encoder.fit_transform(train_data[significant_categoricals]))
cat_one_hot.columns = encoder.get_feature_names_out(significant_categoricals)

# =============== Analisi di correlazione ===============
# Matrice di correlazione (tutte le variabili numeriche)
numeric_data = train_data.select_dtypes(include=[np.number])
plt.figure(figsize=(16, 12))
sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Matrice di correlazione')
plt.show()

# Scatter plot tra le variabili più correlate e SalePrice
top_corr_features = cor_target.index[1:6].intersection(numeric_data.columns)
plt.figure(figsize=(14, 10))
for i, feature in enumerate(top_corr_features, 1):
    plt.subplot(3, 2, i)
    sns.scatterplot(x=numeric_data[feature], y=numeric_data['SalePrice'])
    plt.title(f'{feature} vs SalePrice')
plt.tight_layout()
plt.show()
# Matrice di correlazione
plt.figure(figsize=(12, 8))
sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, mask=np.triu(np.ones(numeric_data.corr().shape)))
plt.title('Matrice di correlazione')
plt.show()

# Scatter plot tra le variabili più correlate e SalePrice
top_corr_features = cor_target.index[1:6]
plt.figure(figsize=(14, 10))
for i, feature in enumerate(top_corr_features, 1):
    plt.subplot(3, 2, i)
    sns.scatterplot(x=numeric_data[feature], y=numeric_data['SalePrice'])
    plt.title(f'{feature} vs SalePrice')
plt.tight_layout()
plt.show()
numeric_data = train_data.select_dtypes(include=[np.number])
cor_target = numeric_data.corr()['SalePrice'].sort_values(ascending=False)
selected_vars = cor_target[abs(cor_target) >= 0.2].index
numeric_data = numeric_data[selected_vars]

# Rimuove NaN e Inf
numeric_data.replace([np.inf, -np.inf], np.nan, inplace=True)
numeric_data.dropna(inplace=True)

# =============== VIF per Multicollinearità ===============
vif = pd.Series([variance_inflation_factor(numeric_data.values, i) 
                 for i in range(numeric_data.shape[1])], 
                index=numeric_data.columns)

high_vif = vif[vif > 10].index
numeric_data.drop(columns=high_vif, inplace=True)

# =============== Unione dataset finale ===============
final_train = pd.concat([numeric_data, cat_one_hot], axis=1)

# Sostituisci NaN con la media
final_train.fillna(final_train.mean(), inplace=True)

# =============== Salvataggio del dataset ===============
os.makedirs('dataset_progetto', exist_ok=True)
final_train.to_csv('dataset_progetto/save_train.csv', index=False)

# =============== Test set ===============
test_data = test_data.iloc[:, 1:]
test_data.fillna('UNWANTED', inplace=True)

cat_one_hot_test = pd.DataFrame(encoder.transform(test_data[significant_categoricals]))
cat_one_hot_test.columns = encoder.get_feature_names_out(significant_categoricals)

numeric_test_data = test_data.select_dtypes(include=[np.number])[selected_vars]
numeric_test_data.drop(columns=high_vif, inplace=True)

final_test = pd.concat([numeric_test_data, cat_one_hot_test], axis=1)
final_test.fillna(final_test.mean(), inplace=True)

final_test.to_csv('dataset_progetto/save_test.csv', index=False)

# =============== Verifica colonne ===============
if list(final_train.columns) == list(final_test.columns):
    print("✅ I dataset hanno le stesse colonne.")
else:
    print("⚠️ I dataset NON hanno le stesse colonne.")

    
